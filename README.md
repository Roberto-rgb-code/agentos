# ğŸ¤– Agentos

**Self-hosted, Local-first, AI-powered Desktop CRM with Workflow Automation**

Una aplicaciÃ³n de escritorio que integra chatbot con IA (Ollama), CRM, Analytics y automatizaciÃ³n de workflows (n8n) â€” todo corriendo localmente en tu mÃ¡quina.

---

## ğŸ“‹ Requisitos

- **Docker Desktop** (v4.0+)
- **Docker Compose** (v2.0+)
- **Ollama** instalado nativamente ([ollama.com](https://ollama.com))
- **Node.js 20** (solo para desarrollo con Electron)
- **Git**

---

## ğŸš€ InstalaciÃ³n rÃ¡pida (Docker)

### 1. Clonar el repositorio

```bash
git clone https://github.com/Roberto-rgb-code/agentos.git
cd agentos
```

### 2. Instalar Ollama (en el host)

```bash
# macOS
brew install ollama

# O descargar desde https://ollama.com

# Descargar modelos
ollama pull llama3.1:8b
ollama pull nomic-embed-text
```

### 3. Levantar todos los servicios

```bash
docker compose -f docker-compose.dev.yml up --build -d
```

Esto levanta:
| Servicio | Puerto | DescripciÃ³n |
|----------|--------|-------------|
| Frontend | 3000 | React UI (nginx) |
| Backend  | 3001 | Node.js API |
| PostgreSQL | 5432 | Base de datos |
| n8n | 5678 | Workflow automation |

### 4. Acceder

- **App web:** http://localhost:3000
- **n8n:** http://localhost:5678
- **API:** http://localhost:3001/api

### 5. Primer uso

1. Abre http://localhost:3000
2. Completa el formulario de registro
3. Crea tu primer workspace
4. Â¡Empieza a chatear con la IA!

---

## ğŸ–¥ï¸ Modo Desktop (Electron)

Para usar como app de escritorio:

### 1. AsegÃºrate de que Docker estÃ¡ corriendo los servicios

```bash
docker compose -f docker-compose.dev.yml up -d
```

### 2. Instalar dependencias de Electron

```bash
cd electron
npm install
```

### 3. Abrir la app

```bash
cd electron
npm run dev
```

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Docker Compose               â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Frontend â”‚  â”‚ Backend  â”‚            â”‚
â”‚  â”‚ (React + â”‚â”€â”€â”‚ (Node.js)â”‚            â”‚
â”‚  â”‚  Nginx)  â”‚  â”‚  :3001   â”‚            â”‚
â”‚  â”‚  :3000   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚                  â”‚
â”‚                     â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   n8n    â”‚  â”‚PostgreSQLâ”‚            â”‚
â”‚  â”‚  :5678   â”‚  â”‚  :5432   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ http://host.docker.internal:11434
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama (host)  â”‚
â”‚  LLM local      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Â¿Por quÃ© Ollama fuera de Docker?**
- Mejor rendimiento (acceso directo a CPU/GPU)
- Sin overhead de contenedores para inferencia
- MÃ¡s fÃ¡cil de gestionar modelos

---

## ğŸ“¦ Servicios incluidos

### ğŸ¤– Chat con IA (Ollama)
- Chat conversacional con LLMs locales
- Soporte para mÃºltiples modelos (llama3.1, qwen2.5, mistral, etc.)
- Embeddings locales para bÃºsqueda semÃ¡ntica
- Sin datos enviados a la nube

### ğŸ“Š CRM
- GestiÃ³n de leads y contactos
- Pipeline de ventas
- Timeline de eventos por lead
- IntegraciÃ³n con WhatsApp

### ğŸ“ˆ Analytics
- Dashboard de KPIs
- MÃ©tricas: leads contactados, calificados, conversiÃ³n, RPR
- Filtros por fecha, canal, usuario

### ğŸ”„ Workflows (n8n)
- AutomatizaciÃ³n de procesos
- IntegraciÃ³n con WhatsApp Cloud API
- Webhooks para recibir datos externos
- ConexiÃ³n directa a la base de datos

---

## ğŸ‘¥ Roles de usuario

| Rol | Permisos |
|-----|----------|
| `admin` | Todo: usuarios, configuraciÃ³n, LLM, CRM, Analytics, Workflows |
| `manager` | CRM, Analytics, gestiÃ³n de usuarios |
| `default` | Chat, CRM bÃ¡sico |

---

## ğŸ”§ ConfiguraciÃ³n

### Variables de entorno principales

Las variables se configuran en `docker-compose.dev.yml`:

| Variable | DescripciÃ³n | Default |
|----------|-------------|---------|
| `DATABASE_URL` | URL de PostgreSQL | `postgresql://agentos:agentos@postgres:5432/agentos_dev` |
| `JWT_SECRET` | Secret para tokens | `agentos-secret-change-in-production` |
| `LLM_PROVIDER` | Proveedor de LLM | `ollama` |
| `OLLAMA_BASE_PATH` | URL de Ollama | `http://host.docker.internal:11434` |
| `OLLAMA_MODEL_PREF` | Modelo preferido | `llama3.1:8b` |
| `EMBEDDING_ENGINE` | Motor de embeddings | `ollama` |
| `EMBEDDING_BASE_PATH` | URL del embedder | `http://host.docker.internal:11434` |
| `EMBEDDING_MODEL_PREF` | Modelo de embeddings | `nomic-embed-text` |

### Cambiar el LLM

Desde la UI:
1. Settings â†’ AI Providers â†’ LLM
2. Selecciona el proveedor (Ollama, OpenAI, Anthropic, etc.)
3. Configura los parÃ¡metros
4. Guarda

---

## ğŸ› ï¸ Desarrollo

### Modo desarrollo (sin Docker para frontend/backend)

```bash
# Terminal 1: Levantar DB y n8n
docker compose -f docker-compose.dev.yml up postgres n8n -d

# Terminal 2: Backend
cd server
yarn install
yarn dev

# Terminal 3: Frontend
cd frontend
yarn install
yarn dev

# Terminal 4 (opcional): Electron
cd electron
npm install
npm run dev
```

### Comandos Ãºtiles

```bash
# Ver logs de todos los servicios
docker compose -f docker-compose.dev.yml logs -f

# Ver logs de un servicio especÃ­fico
docker compose -f docker-compose.dev.yml logs -f server

# Reiniciar un servicio
docker compose -f docker-compose.dev.yml restart server

# Parar todo
docker compose -f docker-compose.dev.yml down

# Parar y borrar volÃºmenes (âš ï¸ borra datos)
docker compose -f docker-compose.dev.yml down -v

# Reconstruir imÃ¡genes
docker compose -f docker-compose.dev.yml up --build -d
```

---

## ğŸ“± IntegraciÃ³n con WhatsApp

1. Configura n8n (http://localhost:5678)
2. Crea un workflow con nodo Webhook
3. Conecta con WhatsApp Cloud API (Meta for Developers)
4. Los mensajes de WhatsApp se reciben â†’ n8n â†’ API â†’ DB â†’ CRM

Para desarrollo local necesitas un tÃºnel pÃºblico:
```bash
# OpciÃ³n A: ngrok
ngrok http 5678

# OpciÃ³n B: Cloudflare Tunnel
cloudflared tunnel --url http://localhost:5678
```

---

## ğŸ“ Estructura del proyecto

```
agentos/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.server      # Backend containerizado
â”‚   â”œâ”€â”€ Dockerfile.frontend    # Frontend containerizado
â”‚   â””â”€â”€ nginx.conf             # Nginx config para frontend
â”œâ”€â”€ docker-compose.dev.yml     # Todos los servicios
â”œâ”€â”€ server/                    # Backend Node.js
â”‚   â”œâ”€â”€ endpoints/             # API endpoints
â”‚   â”œâ”€â”€ models/                # Modelos de datos
â”‚   â”œâ”€â”€ prisma/                # Schema y migraciones
â”‚   â””â”€â”€ utils/                 # Utilidades
â”œâ”€â”€ frontend/                  # React + Vite
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/        # Componentes React
â”‚       â”œâ”€â”€ pages/             # PÃ¡ginas (CRM, Analytics, Workflows)
â”‚       â””â”€â”€ utils/             # Utilidades
â”œâ”€â”€ electron/                  # App desktop (Electron)
â”‚   â”œâ”€â”€ main.js                # Proceso principal
â”‚   â””â”€â”€ preload.js             # Preload script
â””â”€â”€ README.md
```

---

## ğŸ› SoluciÃ³n de problemas

### Los contenedores no arrancan
```bash
docker compose -f docker-compose.dev.yml logs
```

### Ollama no responde
```bash
# Verificar que Ollama estÃ© corriendo
curl http://localhost:11434/api/tags

# Si no responde, iniciar Ollama
ollama serve
```

### Error de conexiÃ³n a la base de datos
```bash
# Verificar que PostgreSQL estÃ© corriendo
docker compose -f docker-compose.dev.yml ps postgres

# Ver logs de PostgreSQL
docker compose -f docker-compose.dev.yml logs postgres
```

### Reconstruir desde cero
```bash
docker compose -f docker-compose.dev.yml down -v
docker compose -f docker-compose.dev.yml up --build -d
```

---

## ğŸ“„ Licencia

MIT

---

## ğŸ¤ Contribuir

1. Fork el repositorio
2. Crea tu feature branch (`git checkout -b feature/mi-feature`)
3. Commit tus cambios (`git commit -m 'Add mi feature'`)
4. Push a la branch (`git push origin feature/mi-feature`)
5. Abre un Pull Request
